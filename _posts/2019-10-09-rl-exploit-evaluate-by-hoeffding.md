---
layout: post
title: Exploit Evaluation By Hoeffding Bounds
---

## Prologue To <font color="Red">Exploit</font> Evaluation By <font color="Red">Hoeffding Bounds</font>
<p class="message">
In MDP model, exploration is executed by state transition, exploitation is proceeded by repeating, looping over one node of state.  The number of exploitation evaluated by using the <font color="Red">Hoeffding bounds</font> would come out the <font color="OrangeRed">low error result of high probability</font>, pervasively found in modern <font color="Red">reinforcement learning</font>.  
</p>

### How Confident We Are In The Seleceted Target Bandit Arm
><font color="DeepSkyBlue">[1]</font>
><font color="RoyalBlue">The question</font>  
>We are still pulling bandit arm, here is the question, how do we believe that we choose the right bandit?  How confident we are in the bandit thus choosed?  
>
><font color="DeepSkyBlue">[2]</font>
><font color="OrangeRed">Recap the Chebyshev's inequality</font>  
>In my prior post [Hoeffding Inequality versus Chebyshev's Inequality]({{ site.github.repo }}{{ site.baseurl }}/2017/10/24/prereq-hoeffding-vs-law-large-number/), we learn that for any random variable $Y$, the probabilistic error for the differrence in between $Y$ and its expected value $E\lbrack Y\rbrack\$ greater than or equal to $\varepsilon$ is smaller than or equal to $\frac {Var\lbrack Y\rbrack\}{\varepsilon^{2}}$:  
>$P(\vert Y-E\lbrack Y\rbrack\vert\geq\varepsilon)\leq\frac {Var\lbrack Y\rbrack\}{\varepsilon^{2}}$  
>
>Step any further, if we take $X_{i}$ for each distinct $i$-th test, and suppose we believe all $X_{i}$, $i$=$1,2,...,n$ are distributed with expected value $\mu$ and variance $\nu^{2}$, then:  
>$P(\vert \widehat\mu-\mu\vert\geq\varepsilon)\leq\frac {\nu^{2}}{n\cdot\varepsilon^{2}}$  
>, where $\widehat\mu$ is the estimator of $\mu$ from all $X_{i}$.  
>
>Here we can say the estimator $\widehat\mu$ is bias within $\pm\varepsilon$ is probabilistically less than or equal to $\frac {\nu^{2}}{n\cdot\varepsilon^{2}}$.  
>
>The point is <font color="RoyalBlue">how many test cases</font>, <font color="RoyalBlue">how many times</font> we should make the test to convince us that we are confident in this result?  
>
><font color="DeepSkyBlue">[3]</font>
><font color="OrangeRed">By using the Hoeffding inequality</font>  
>My prior post [Hoeffding Bounds]({{ site.github.repo }}{{ site.baseurl }}/2018/03/08/prob-bound-hoeffding-bound/) has proved that it holds for $X_{i}$, $i$=$1,2,...,n$, each <font color="OrangeRed">weak dependent</font> or <font color="OrangeRed">identical independent distributed</font> random variables to limit its upper bound:  
>$P(\vert\sum_{i=1}^{n}(Z_i)\vert\ge n\cdot\varepsilon)\le$ <font color="Red">2</font>$\cdot exp(\frac {-2\cdot n\cdot\varepsilon^2}{(b-a)^2})$  
>$\Rightarrow P(\vert\sum_{i=1}^{n}(X_i-E\lbrack X_i\rbrack)\vert\ge n\cdot\varepsilon)\le$ <font color="Red">2</font>$\cdot exp(\frac {-2\cdot n\cdot\varepsilon^2}{(b-a)^2})$  
>$\Rightarrow P(\vert\frac {1}{n}\cdot\sum_{i=1}^{n}(X_i-E\lbrack X_i\rbrack)\vert\ge\varepsilon)\le$ <font color="Red">2</font>$\cdot exp(\frac {-2\cdot n\cdot\varepsilon^2}{(b-a)^2})$  
>, where  
>&#10112;$Z_i$=$X_i-E\lbrack X_i\rbrack$ for all $i$  
>&#10113;<font color="OrangeRed">$Z_i\in\lbrack a,b\rbrack$</font>  
>
>In this <font color="Red">Bernoulli</font> bandit arm distribution, we take $a=0$ and $b=1$, the whole <font color="OrangeRed">average bias</font> of inequality becomes:  
>$P(\vert\frac {1}{n}\cdot\sum_{i=1}^{n}(X_i-E\lbrack X_i\rbrack)\vert\ge\varepsilon)\le$ <font color="Red">2</font>$\cdot exp(-2\cdot n\cdot\varepsilon^2)$  
>
>Now we treat the $i$-th pull of this same bandit arm $X_{i}$=$\{0,1\}$, and take $u$ to be the average estimated value, take $v$ to be the average expected value, which is the common <font color="OrangeRed">Hoeffding inequality</font> expression:  
>$P(\vert u-v\vert\ge\varepsilon)\le$ <font color="Red">2</font>$\cdot exp(-2\cdot n\cdot\varepsilon^2)$  
>
>We could formally claim our bandit arm experiment finally brings us the right arm with the error bias to the true optimal arm limited to $\varepsilon$ is probabilistically no more than <font color="Red">2</font>$\cdot exp(-2\cdot n\cdot\varepsilon^2)$.  
>
>If we choose $\delta$ to be this error of probability, then  
>$\delta$=<font color="Red">2</font>$\cdot exp(-2\cdot n\cdot\varepsilon^2)$  
>$\Rightarrow \varepsilon$=$(\frac {1}{2}\cdot ln\frac {2}{\delta})^{\frac {1}{2}}/n^{\frac {1}{2}}$  
>$\Rightarrow n$=$\frac {1}{2}\cdot ln\frac {2}{\delta}/\varepsilon^{2}$  

### Addendum
>&#10112;[Exploring Exploration, Charles IsBell, Michael Littman, Reinforcement Learning By Georgia Tech(CS8803)](https://classroom.udacity.com/courses/ud600/lessons/4402978778/concepts/44548888230923)  

<!-- Γ -->
<!-- \Omega -->
<!-- \cap intersection -->
<!-- \cup union -->
<!-- \frac{\Gamma(k + n)}{\Gamma(n)} \frac{1}{r^k}  -->
<!-- \mbox{\large$\vert$}\nolimits_0^\infty -->
<!-- \vert_0^\infty -->
<!-- \vert_{0.5}^{\infty} -->
<!-- &prime; ′ -->
<!-- &Prime; ″ -->
<!-- $E\lbrack X\rbrack$ -->
<!-- \overline{X_n} -->
<!-- \underset{Succss}P -->
<!-- \frac{{\overline {X_n}}-\mu}{S/\sqrt n} -->
<!-- \lim_{t\rightarrow\infty} -->
<!-- \int_{0}^{a}\lambda\cdot e^{-\lambda\cdot t}\operatorname dt -->
<!-- \Leftrightarrow -->
<!-- \prod_{v\in V} -->
<!-- \subset -->
<!-- \subseteq -->
<!-- \varnothing -->
<!-- \perp -->
<!-- \overset\triangle= -->
<!-- \left|X\right| -->
<!-- \xrightarrow{r_t} -->
<!-- \left\|?\right\| => ||?||-->
<!-- \left|?\right| => |?|-->
<!-- \lbrack BQ\rbrack => [BQ] -->
<!-- \subset -->
<!-- \subseteq -->

<!-- Notes -->
<!-- <font color="OrangeRed">items, verb, to make it the focus, mathematic expression</font> -->
<!-- <font color="Red">KKT</font> -->
<!-- <font color="Red">SMO heuristics</font> -->
<!-- <font color="Red">F</font> distribution -->
<!-- <font color="Red">t</font> distribution -->
<!-- <font color="DeepSkyBlue">suggested item, soft item</font> -->
<!-- <font color="RoyalBlue">old alpha, quiz, example</font> -->
<!-- <font color="Green">new alpha</font> -->

<!-- <font color="#C20000">conclusion, finding</font> -->
<!-- <font color="DeepPink">positive conclusion, finding</font> -->
<!-- <font color="RosyBrown">negative conclusion, finding</font> -->

<!-- <font color="#00ADAD">policy</font> -->
<!-- <font color="#6100A8">full observable</font> -->
<!-- <font color="#FFAC12">partial observable</font> -->
<!-- <font color="#EB00EB">stochastic</font> -->
<!-- <font color="#8400E6">state transition</font> -->
<!-- <font color="#D600D6">discount factor gamma $\gamma$</font> -->
<!-- <font color="#D600D6">$V(S)$</font> -->
<!-- <font color="#9300FF">immediate reward R(S)</font> -->

<!-- ### <font color="RoyalBlue">Example</font>: Illustration By Rainy And Sunny Days In One Week -->
<!-- <font color="RoyalBlue">[Question]</font> -->
<!-- <font color="DeepSkyBlue">[Answer]</font> -->

<!-- <font color="Brown">Notes::mjtsai1974</font> -->

<!-- 
[1]Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?</font>  
-->

<!--
><font color="DeepSkyBlue">[Notes]</font>
><font color="OrangeRed">Why at this moment, the Poisson and exponential probability come out with different result?</font>  
-->

<!-- https://www.medcalc.org/manual/gamma_distribution_functions.php -->
<!-- https://www.statlect.com/probability-distributions/student-t-distribution#hid5 -->
<!-- http://www.wiris.com/editor/demo/en/ -->