---
layout: post
title: Introduction To The Moment Generating Function
---

## Prologue To The Moment Generating Function
<p class="message">
In probability theory and statistics, the moment generating function(MGF) of a real-valued random variable is an alternative specification of its probability distribution.  Caution must be made that not all random variables 
have moment generating functions.  This article introduce to MGF with a hope that it could fasten the way to generalize the expectation and variance of a random variable with regards to its given PDF(contiguous) or PMF(discrete) 
by means of <font color="Red">moment</font>.
</p>

### What is a <font color="Red">Moment</font>?
>The expected values $E\lbrack X\rbrack$, $E\lbrack X^2\rbrack$, $E\lbrack X^3\rbrack$,..., and $E\lbrack X^r\rbrack$ are called <font color="Red">moments</font>.  As you might already have explored in the statistics related reference and have it that:  
>&#10112;the mean, $\mu=E\lbrack X\rbrack$.  
>&#10113;the variance, $\sigma^2=Var\lbrack X\rbrack=E\lbrack X^2\rbrack-E^2\lbrack X\rbrack$.  
>
>They are called the <font color="Red">functions of moments</font>, sometimes are difficult to found.  The moment generating function provides an add-in in finding the k-th ordinary moment, the mean, and even more.  

### What is an <font color="Red">MGF</font>?
>The <font color="Red">MGF(moment generating function)</font> of a random variable X, say it discrete, is usually given by:  
>$M_X(t)=E\lbrack e^{t\cdot X}\rbrack$  
>$\;\;\;\;\;\;\;\;=E\lbrack 1+\frac{t\cdot X}{1!}+\frac{(t\cdot X)^2}{2!}+\cdots+\frac{(t\cdot X)^k}{k!}+\cdots\rbrack$  
>Where $E\lbrack e^{t\cdot X}\rbrack$ exists in $\lbrack -h, h\rbrack$ for some $h$.  
>$E\lbrack X\rbrack$, called the first ordinary moment of random variable $X$.  More precisely, denoted by $\mu_1$.  
>$E\lbrack X^2\rbrack$, also called the second ordinary moment of random variable $X$, denoted by $\mu_2$.  
>$E\lbrack X^k\rbrack$, also called the k-th ordinary moment of random variable $X$, denoted by $\mu_k$.  
>
>$e^{t\cdot X}=1$+$\frac{t\cdot X}{1!}$+$\frac{(t\cdot X)^2}{2!}$+$\cdots$+$\frac{(t\cdot X)^k}{k!}$+$\cdots$  
>By above Taylor series, the coefficient of $X^k$ is $\frac{t^k}{k!}$, hence, if $M_X(t)$ exists for $-h<t<h$, there must exist a mapping between the random variable $X$ and $e^{t\cdot X}$.  
>
>If <font color="DeepPink">$p(X)$ and $p(Y)$ has $M_X(t)=M_Y(t)$</font>, that is to say <font color="DeepPink">$E\lbrack e^{t\cdot X}\rbrack$=$\lbrack e^{t\cdot Y}\rbrack$,then, the distribution of random variable $X$ and $Y$ are the same</font>.  
>
>We also have it that $E\lbrack e^{i\cdot t\cdot X}\rbrack$=$e^i\cdot$ $E\lbrack e^{t\cdot X}\rbrack$.  You can make the deduction yourself.  

### The Deduction Of The <font color="Red">$\mu_k$</font>
>There exists a lof many peoperties of the <font color="Red">MGF</font>, but the majority of this article focus on the way to find out the k-th ordinary moment.  Next to deduce the discrete MGF to express the moment.  
>
>&#10112;  
>$M_X(t)=E\lbrack e^{t\cdot X}\rbrack$  
>$\;\;\;\;\;\;\;\;=E\lbrack\sum_{k=0}^\infty\frac{(t\cdot X)^k}{k!}\rbrack$  
>$\;\;\;\;\;\;\;\;=\sum_{k=0}^\infty\frac{E\lbrack X^k\rbrack\cdot t^k}{k!}$  
>where $E\lbrack X^k\rbrack$=$\mu_k$=$\sum_{i=1}^{\infty}(x_i)^k\cdot p(x_i)$, and $X=\{x_1,x_2,x_3,\cdots\}$,  
>$\;\;\;\;\;\;\;\;=\sum_{k=0}^\infty\frac{\mu_k\cdot t^k}{k!}$  
>$\;\;\;\;\;\;\;\;=\sum_{k=0}^\infty\frac{\lbrack\sum_{i=1}^{\infty}(x_i)^k\cdot p(x_i)\rbrack\cdot t^k}{k!}$  
>$\;\;\;\;\;\;\;\;=\sum_{i=1}^\infty\sum_{k=0}^{\infty}\frac{(x_i)^k\cdot t^k}{k!}\cdot p(x_i)$  
>$\;\;\;\;\;\;\;\;=\sum_{i=1}^\infty e^{t\cdot x_i}\cdot p(x_i)$  
>
>&#10113;  
>$E\lbrack e^{t\cdot X}\rbrack=\sum_{i=1}^\infty e^{t\cdot x_i}\cdot p(x_i)$  
>$\;\;\;\;\;\;\;\;=\sum_{i=1}^\infty(1$+$\frac{t\cdot x_i}{1!}$+$\frac{(t\cdot x_i)^2}{2!}$+$\cdots$+$\frac{(t\cdot x_i)^k}{k!}$+$\cdots)\cdot p(x_i)$  
>$\;\;\;\;\;\;\;\;=\sum_{i=1}^\infty 1\cdot p(x_i)$+$\sum_{i=1}^\infty\frac{t\cdot x_i}{1!}\cdot p(x_i)$+$\sum_{i=1}^\infty\frac{(t\cdot x_i)^2}{2!}\cdot p(x_i)$+$\cdots$+$\sum_{i=1}^\infty\frac{(t\cdot x_i)^k}{k!}\cdot p(x_i)$+$\cdots$    
>$\;\;\;\;\;\;\;\;=1$+$t\cdot\sum_{i=1}^\infty x_i\cdot p(x_i)$+$\frac{t^2}{2!}\cdot\sum_{i=1}^\infty (x_i)^2\cdot p(x_i)$+$\cdots$+$\frac{t^k}{k!}\cdot\sum_{i=1}^\infty (x_i)^k\cdot p(x_i)$+$\cdots$  
>$\;\;\;\;\;\;\;\;=1$+$t\cdot E\lbrack X\rbrack$+$\frac{t^2}{2!}\cdot E\lbrack X^2\rbrack$+$\cdots$+$\frac{t^k}{k!}\cdot E\lbrack X^k\rbrack$+$\cdots$  
>$\;\;\;\;\;\;\;\;=1$+$t\cdot \mu_1$+$\frac{t^2}{2!}\cdot \mu_2$+$\cdots$+$\frac{t^k}{k!}\cdot\mu_k$+$\cdots$  

<!-- Notes -->
<!-- <font color="OrangeRed">items, verb, to make it the focus</font> -->
<!-- <font color="Red">KKT</font> -->
<!-- <font color="Red">SMO heuristics</font> -->
<!-- <font color="DeepSkyBlue">suggested item, soft item</font> -->
<!-- <font color="RoyalBlue">old alpha</font> -->
<!-- <font color="Green">new alpha</font> -->

<!-- <font color="DeepPink">positive conclusion, finding</font> -->
<!-- <font color="DimGray">negative conclusion, finding</font> -->

<!-- <font color="#00ADAD">policy</font> -->
<!-- <font color="#6100A8">full observable</font> -->
<!-- <font color="#FFAC12">partial observable</font> -->
<!-- <font color="#EB00EB">stochastic</font> -->
<!-- <font color="#8400E6">state transition</font> -->
<!-- <font color="#D600D6">discount factor gamma $\gamma$</font> -->
<!-- <font color="#D600D6">$V(S)$</font> -->
<!-- <font color="#9300FF">immediate reward R(S)</font> -->