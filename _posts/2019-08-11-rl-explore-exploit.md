---
layout: post
title: Explore versus Exploit
---

## Prologue To <font color="Red">Explore</font> versus <font color="Red">Exploit</font>
<p class="message">
Some reference of articles states that exploration is the topic that separates reinforcement learning from other kinds of machine learning.  Exploration is part of fundamental trade off of reinforcement learning.
</p>

### Begin from K-Armed Bandits
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">K-armed bandits</font>  
>Supposed we are given K-armed bandits, where $K$=$7$ in below exhibition.  <font color="RosyBrown">The bandit has no state transition, all about stochasticity</font>, you pull it, you either get a jackpot, or you don't get a jackpot.  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2019-08-11-rl-explore-exploit-k-armed-bandits.png "k-armed bandits")
>
><font color="DeepSkyBlue">[2]</font>
><font color="RoyalBlue">Which one you will pull?</font>  
>In this example, suppose each machine will have different paypffs.  The question is <font color="RoyalBlue">which bandit will you pull?</font>  
>&#10112;the one that has the best chance of winning the jackpot, giving you money.  
>&#10113;the point is that we don't know the payoffs are, hard to decide which bandit to choose.  
>&#10114;if we know the payoffs, just do the $max_{arm}Bandit(arm)$ could we get the answer.  
>
><font color="DeepSkyBlue">[3]</font>
><font color="DeepPink">we have to explore</font>  
>Suppose each bandit has some probability of paying off a one, some probability of paying off nothing, saying that zero.  But, we don't know what it is at first.  
>
>If we pull bandit a, and it doesn't pay off, does it mean that bandit a is not the best one?  <font color="RosyBrown">No</font>, we don't know what anything else is.  
>
>One arm pull just give us some information, but not an awful lot.  We have to combine information across a whole lot of pulls to start to evaluate what the best bandit is.  
>
>So, <font color="DeepPink">we have to explore</font>!!  
>
><font color="DeepSkyBlue">[4]</font>
><font color="OrangeRed">Illustration</font>  
>Suppose each of these bandits has been pulled multiple times and come out the number of times that it pays off 1, with the fomer in the denominator, the later in the nominator, given in below exhibition.  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2019-08-11-rl-explore-exploit-k-armed-bandits-ex.png "ex")
>In between a and e, it looks as if that bandit e is better than bandit a, since the same 10 pulls of bandit, e has payed 2 times, while a just 1 time.  
>
>Next to examine in between a and d, the bandit d gave the pay off only after 5 pulls, it might also be plausible if we prefer to bandit d rather than a.  
>
>By the given data, we have 2 concerns:  
>&#10112;which bandit is the <font color="OrangeRed">highest expected</font> pay off?  
>&#10113;which are we <font color="OrangeRed">most confident</font> in?  
>
>For &#10112;, the bandits d,e,f,g just play to a tie, since  
>$\frac {1}{5}\cdot 1$+$\frac {4}{5}\cdot 0$,$\frac {2}{10}\cdot 1$+$\frac {8}{10}\cdot 0$,$\frac {4}{20}\cdot 1$+$\frac {16}{20}\cdot 0$, $\frac {8}{40}\cdot 1$+$\frac {32}{40}\cdot 0$, are the expected pay off for bandit d,e,f,g respectively.  They are all the same higher than the rest.  
>
>For &#10113;, as to the confidence level, if you choose g, it might be a good answer, but <font color="RosyBrown">not the good reason</font>.  <font color="DeepPink">When it comes to expectation, the confidence level is monotonically increasing function of the numbers of sample.</font>  so, <font color="DeepPink">whichever has the biggest pulling numbers should be the most confident</font>, because it has most samples.  We finally compare c and g, and choose g.  

### Confidence-Based Exploration
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">Something else in stochasticity</font>  
>Given 2 bandits a and b, you make 1 pull of a and get 0 pay off, whereas, you make 2 pulls of b and get 1 pay off.  Do you think the best thing at this point is to <font color="OrangeRed">always</font> pick up a, <font color="OrangeRed">always</font> pick up b, or <font color="DeepPink">something else</font>?  
>
>Trivially, <font color="DeepPink">something else</font>.  
>
>First, let's calculate out the probability for:  
>&#10112;always a to get 0 pay off  
>$\frac {1}{C_{1}^{3}}\cdot 1$=$\frac {1}{3}$  
>Since there exists 1 pull of bandit a out of the 3 pulls, and it indeed come out with 0 pay off.  
>
>&#10113;always b to get 1 pay off  
>$\frac {1}{C_{2}^{3}}\cdot 2$=$\frac {2}{3}$  
>We treat 3 pulls as distinct computation unit, by given, there exists 2 pulls of bandit b and come out with 1 pay off.  Totally 2 out of 3 pulls, we prefer all bandit b, where each pull is not replacable, and the 1 pay off might be appeared in the 1st or 2nd pull, that's why we multiply by 2 at the end.  
>
>Suppose you choose bandit b by intuition, you are right $\frac {2}{3}$ of the time, but with $\frac {1}{3}$ of chance you make mistake.  That's why we end up with <font color="DeepPink">something else</font>.  
>
><font color="DeepSkyBlue">[2]</font>
><font color="RoyalBlue">What is something else?</font>  
>If we run an arbitrary number of pulls, what is something else?  I think, we might just evaluate which bandit to pull after some time, or infinite amount of time later.  Then, I might know the true expected value for pulling bandit a or b, so I know which to choose.  
>
>It refers to the thing I'd like to do to get me <font color="DeepPink">more confidence in my expectation</font>, and more number of pulls explicitly.  Get more confidence in expectation would be the next step:  
>&#10112;maximum likelihood  
>&#10113;maximum confidence level  
>&#10114;minimum confidence level  
>
><font color="DeepSkyBlue">[3]</font>
><font color="OrangeRed">Maximum likelihood</font>  
>The <font color="OrangeRed">maximum likelihood</font> over here is <font color="OrangeRed">to find out the bandit which has the highest expected pay off</font>, and <font color="OrangeRed">choose it</font>, then <font color="OrangeRed">keep revisiting this question</font>.  
>
>Each time you pull the bandit, recalculate your expected values again and again.  choose the highest pay off bandit and continue so on.  
>
>As new data of next pull comes in, we will refactor our calculation and make new decision again, but, that's <font color="RosyBrown">not</font> a good approach.  
>
>By the given case, bandit b is the maximum likelihood, say $MLE_{a}$=$0$,$MLE_{b}$=$0.5$.  In this test of maximum likelihood, you will just choose b as beginning, since $MLE_{b}$>$MLE_{a}$.  
>
>When I choose b for the very first time, whatever outcome of the result, the expected pay off of b is much higher than bandit a.  Next, I continue to choose b, it still has expected pay off higher than 0, such behavior would be elapsed over a long long period would just fall into the expectation.  And, it will begin to approach to 0 after an infinite amount of time.  
>
>The major point is that even if I choos b and it never pay off, it still has non-zero expected pay off, which implies that I never have the chance to choose a, it is equivalent to say that $\frac {2}{3}$ of the time you are right, but $\frac {1}{3}$ of the time you make mistake.  
>
>We can do better than that.  
>
><font color="DeepSkyBlue">[4]</font>
><font color="OrangeRed">Maximum confidence level</font>  
>The <font color="OrangeRed">maximum confidence level</font> is the strategy for whichever one you have the <font color="OrangeRed">best estimate</font>, you should choose that one.  But, that's the <font color="RosyBrown">bad</font> idea, too.  <font color="RoyalBlue">Why?</font>  
>
>Because whichever one I start out must have the <font color="OrangeRed">most(current maximum)</font> confidence level of positive result, which implies that I will have <font color="OrangeRed">more and more</font> data to further <font color="OrangeRed">consolidate</font> the <font color="OrangeRed">current maximum confidence one</font>.  
>
>Although <font color="OrangeRed">current maximum</font> confidence level of positive result bandit <font color="RosyBrown">might not guarantee its future pay off</font>, <font color="RosyBrown">it still can keep non-zero expected payoff over some period of time</font>.  In this example, the strategy will always choose b.  

### Addendum
>&#10112;[Exploring Exploration, Charles IsBell, Michael Littman, Reinforcement Learning By Georgia Tech(CS8803)](https://classroom.udacity.com/courses/ud600/lessons/4402978778/concepts/44548888230923)  

<!-- Γ -->
<!-- \Omega -->
<!-- \cap intersection -->
<!-- \cup union -->
<!-- \frac{\Gamma(k + n)}{\Gamma(n)} \frac{1}{r^k}  -->
<!-- \mbox{\large$\vert$}\nolimits_0^\infty -->
<!-- \vert_0^\infty -->
<!-- \vert_{0.5}^{\infty} -->
<!-- &prime; ′ -->
<!-- &Prime; ″ -->
<!-- $E\lbrack X\rbrack$ -->
<!-- \overline{X_n} -->
<!-- \underset{Succss}P -->
<!-- \frac{{\overline {X_n}}-\mu}{S/\sqrt n} -->
<!-- \lim_{t\rightarrow\infty} -->
<!-- \int_{0}^{a}\lambda\cdot e^{-\lambda\cdot t}\operatorname dt -->
<!-- \Leftrightarrow -->
<!-- \prod_{v\in V} -->
<!-- \subset -->
<!-- \subseteq -->
<!-- \varnothing -->
<!-- \perp -->
<!-- \overset\triangle= -->
<!-- \left|X\right| -->
<!-- \xrightarrow{r_t} -->
<!-- \left\|?\right\| => ||?||-->
<!-- \left|?\right| => |?|-->
<!-- \lbrack BQ\rbrack => [BQ] -->
<!-- \subset -->
<!-- \subseteq -->

<!-- Notes -->
<!-- <font color="OrangeRed">items, verb, to make it the focus, mathematic expression</font> -->
<!-- <font color="Red">KKT</font> -->
<!-- <font color="Red">SMO heuristics</font> -->
<!-- <font color="Red">F</font> distribution -->
<!-- <font color="Red">t</font> distribution -->
<!-- <font color="DeepSkyBlue">suggested item, soft item</font> -->
<!-- <font color="RoyalBlue">old alpha, quiz, example</font> -->
<!-- <font color="Green">new alpha</font> -->

<!-- <font color="#C20000">conclusion, finding</font> -->
<!-- <font color="DeepPink">positive conclusion, finding</font> -->
<!-- <font color="RosyBrown">negative conclusion, finding</font> -->

<!-- <font color="#00ADAD">policy</font> -->
<!-- <font color="#6100A8">full observable</font> -->
<!-- <font color="#FFAC12">partial observable</font> -->
<!-- <font color="#EB00EB">stochastic</font> -->
<!-- <font color="#8400E6">state transition</font> -->
<!-- <font color="#D600D6">discount factor gamma $\gamma$</font> -->
<!-- <font color="#D600D6">$V(S)$</font> -->
<!-- <font color="#9300FF">immediate reward R(S)</font> -->

<!-- ### <font color="RoyalBlue">Example</font>: Illustration By Rainy And Sunny Days In One Week -->
<!-- <font color="RoyalBlue">[Question]</font> -->
<!-- <font color="DeepSkyBlue">[Answer]</font> -->

<!-- <font color="Brown">Notes::mjtsai1974</font> -->

<!-- 
[1]Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?</font>  
-->

<!--
><font color="DeepSkyBlue">[Notes]</font>
><font color="OrangeRed">Why at this moment, the Poisson and exponential probability come out with different result?</font>  
-->

<!-- https://www.medcalc.org/manual/gamma_distribution_functions.php -->
<!-- https://www.statlect.com/probability-distributions/student-t-distribution#hid5 -->
<!-- http://www.wiris.com/editor/demo/en/ -->