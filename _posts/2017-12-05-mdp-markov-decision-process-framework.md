---
layout: post
title: Markov Decision Process Framework
---

## Markov Decision Process Framework

<p class="message">
The MDP is quiet beautiful a framework, rather than using a single sequence of states and actions, as would be the case in the deterministic planning, now we make an entire field a so called <font color="#00ADAD">policy</font> that assigns an <font color="DeepSkyBlue">action</font> to every possible <font color="Red">state</font>.  And, we compute it using the technique of <font color="Green">value iteration</font>.
</p>

### Conclusion
>

<!-- Notes -->
<!-- <font color="OrangeRed">items, verb, to make it the focus</font> -->
<!-- <font color="Green">value iteration</font> -->
<!-- <font color="#00ADAD">policy</font> -->
<!-- <font color="#6100A8">full observable</font> -->
<!-- <font color="#FFAC12">partial observable</font> -->
<!-- <font color="#EB00EB">stochastic</font> -->
<!-- <font color="#8400E6">state transition</font> -->
<!-- <font color="#D600D6">discount factor gamma $\gamma$</font> -->
<!-- <font color="DeepSkyBlue">optimal action</font> -->
<!-- <font color="red">value of a state</font> -->
<!-- <font color="#D600D6">$V(S)$</font> -->
<!-- <font color="#9300FF">immediate reward R(S)</font> -->
<!-- <font color="#C20000">positive conclusion, finding</font> -->
<!-- <font color="green">negative conclusion, finding</font> -->
