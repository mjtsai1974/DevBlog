---
layout: post
title: Explore versus Exploit - Part 2
---

## Prologue To <font color="Red">Explore</font> versus <font color="Red">Exploit</font> - Part 2
<p class="message">
Next would be to get the metrics to construct the fundamental basis of algorithm and validate the bandit arms strategy.
</p>

### <font color="OrangeRed">Metrics</font> For Bandit Arm Selection
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">Build some metrics</font>  
>Still we are using the same bandit arms for illustration.  To build an algorithm that can make the best bandit arm selection, always a or always b, we need to build some metrics:  
>&#10112;Identify the optimal bandit arm <font color="OrangeRed">in the limit</font>.    
>&#10113;maximize the <font color="#D600D6">discounted expected reward</font>.    
>&#10114;maximize the <font color="#D600D6">discounted expected reward</font> over <font color="OrangeRed">a finite horizon</font>.  
>&#10115;identify the $\varepsilon$ <font color="#00ADAD">near optimal arm</font> with high probability $1-\delta$ in the number of steps $T(K,\varepsilon,\delta)$, polynominal in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$...[A]  
>&#10116;find the $\varepsilon$ <font color="#9300FF">nearly maximized reward</font> with high probability $1-\delta$ in the number of steps $N(K,\varepsilon,\delta)$, polynominal in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$...[B]  
>&#10117;pull an $\varepsilon$ <font color="RosyBrown">non-near optimal</font> or <font color="RosyBrown">near sub-optimal</font> bandit arm with high probability $1-\delta$ in no more than the number of steps $N^{\'}(K,\varepsilon,\delta)$ times, polynominal in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$...[C]  
>
>Above metrics explicitly lead to the construction of <font color="Red">reinforcement learning</font> which is the fashion we'd like the algorithm to be executed with, where $K$ is he number of bandit arms.  
>
><font color="DeepSkyBlue">[2]</font>
><font color="#00ADAD">Identify the $\varepsilon$ near optimal arm...[A]</font>  
>&#10112;<font color="#00ADAD">$\varepsilon$ near optimal</font> <font color="RosyBrown">doesn't</font> mean that the arm we get is the <font color="RosyBrown">absolute best</font> arm.  
>&#10113;we might be unlucky over a long long period and get a pull paid arm. It would be impossible to get the best arm within finite period of time.  
>
>For above constraints, what we want with [A] is to get a near optimal arm with only $\varepsilon$ distance from it, it might <font color="RosyBrown">not</font> workable all the time, but works with hight probability $1-\delta$.  
>
>The $T(K,\varepsilon,\delta)$ stands for the number of pulls before we get or have identified the near optimal arm, and it must be polynomial in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$.  
>
>Indeed, <font color="OrangeRed">$T(K,\varepsilon,\delta)$ is a polynomial that bounds the number of steps/pulls before we get or have identified the near optimal arm</font>.  
>
><font color="DeepSkyBlue">[3]</font>
><font color="#9300FF">Find the $\varepsilon$ nearly maximized reward...[B]</font>  
>It is a metric similar to [A], after [B] has been executed over the number of steps $N(K,\varepsilon,\delta)$, we can get the $\varepsilon$ <font color="#9300FF">nearly maximized reward</font>.  
>
>$N(K,\varepsilon,\delta)$ is the the number of pulls/steps, for which I run that long with [B], I can get <font color="#9300FF">$\varepsilon$ nearly maximized reward</font>, with high probability $1-\delta$.  
>
>Such number of steps $N(K,\varepsilon,\delta)$ must be polynomial in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$.  
>
>[B] puts more care on <font color="OrangeRed">how we reach the optimal arm</font>, <font color="RosyBrown">not</font> just we get the optimal arm.  It focus more directly with rewards, for <font color="RosyBrown">[A]</font> might do a lot of unfortune trials with <font color="RosyBrown">zero or a few pay off</font>, we <font color="OrangeRed">need to evaluate if current accumulated rewards is sufficient to be the incentive to keep going on in the same direction</font>.  
>
><font color="DeepSkyBlue">[4]</font>
><font color="RosyBrown">Pull an $\varepsilon$ non-near optimal(naer sub-optimal) arm...[C]</font>  
>This is a <font color="OrangeRed">mistake bound</font>.  Inevitably, we might pull <font color="RosyBrown">non-near optimal</font> arm, the mistake arm.  
>
>We want to limit the number of pulls of such <font color="RosyBrown">non-near optimal</font> or <font color="RosyBrown">sub-optimal</font> arm to $N^{\'}(K,\varepsilon,\delta)$, which is a small number.  
>
>It could be over a very very long time window, but the number of mistake pulls is really small, with high probability $1-\delta$.  
>
><font color="DeepSkyBlue">[5]</font>
><font color="OrangeRed">A brief summary - PAC</font>  
>&#10112;[A] focus more on identifying the best output, the near optimal arm, the <font color="Red">exploitation</font>.  
>&#10113;[B] is the accumulated return of rewards, which is the <font color="Red">exploration</font>.  
>&#10114;[C] is the <font color="OrangeRed">mistake bound</font>.  
>
>This looks like <font color="OrangeRed">PAC</font>(<font color="OrangeRed">Probabily Approximately Correct</font>) learning, some paper refer it <font color="OrangeRed">PAO</font>(<font color="OrangeRed">Probabily Approximately Optimal</font>).  Even though above metrics are not fully correct, if the goal is to identify the best arm, we could regard it as <font color="OrangeRed">classification</font>.  

### The Equilibrium Of Metrics [A] And [B] And [C]
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">The target of proof</font>  
>The very next thing is to prove the <font color="OrangeRed">transitivity</font> in metrices [A],[B] and [C], that is to say:  
>$\;\;$Find Best$\Leftrightarrow$Low Mistakes$\Leftrightarrow$Doing Well  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2019-09-04-rl-explore-exploit-part2.png "equal")
>
><font color="DeepSkyBlue">[2]</font>
><font color="OrangeRed">Find Best$\Rightarrow$Low Mistakes</font>  
>&#10112;the beginning:  
>Given that you have an algorithm [A] that can identify <font color="#00ADAD">$\varepsilon$ near optimal arm</font> with high probability $1-\delta$ in the <font color="OrangeRed">bounded number of pulls</font> $T(K,\varepsilon,\delta)$, polynominal in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$.  
>&#10113;next target:  
>We'd like to build another algorithm of <font color="RosyBrown">low mistakes</font>, the number of trials at which it pulls a <font color="RosyBrown">non $\varepsilon'$ close</font>(<font color="RosyBrown">mistake</font>) arm is <font color="OrangeRed">bounded</font> by $T^{\'}(K,\varepsilon^{\'},\delta^{\'})$, where <font color="OrangeRed">$\varepsilon'\geq\varepsilon$</font> and is high probability $1-\delta^{\'}$.  
>  
>proof:  
>$\Rightarrow$first by using algorithm [A], within <font color="OrangeRed">bounded number of pulls</font> $T(K,\varepsilon,\delta)$, we can find <font color="#00ADAD">$\varepsilon$ near optimal arm</font> with high probability $1-\delta$.  
>
>$\Rightarrow$then, <font color="RoyalBlue">what's the total number of mistakes [A] can make?</font>  
>Since it is the <font color="OrangeRed">bounded number of pulls</font>, after that, we can find <font color="#00ADAD">$\varepsilon$ near optimal arm</font>, that is to say <font color="RosyBrown">no more than $T(K,\varepsilon,\delta)$</font>.  
>
>$T^{\'}(K,\varepsilon^{\'},\delta^{\'})\leq T(K,\varepsilon,\delta)$  
>$\Rightarrow$<font color="RoyalBlue">why $T'(K,\varepsilon',\delta')$ is the total number of mistakes?</font>  
>Because the idea of [A] is <font color="#00ADAD">$\varepsilon$ near optimal</font> arm, and the execution of [A] comes out the <font color="RosyBrown">non $\varepsilon'$ close</font>(or say <font color="#00ADAD">$\varepsilon'$ near optimal</font>) arm, where <font color="OrangeRed">$\varepsilon'\geq\varepsilon$</font>.  And we treat the <font color="RosyBrown">non $\varepsilon'$ close</font>(or say <font color="#00ADAD">$\varepsilon'$ near optimal</font>) arm as the <font color="RosyBrown">mistake</font> arm.  
>
>$\Rightarrow$we thus prove <font color="OrangeRed">find best[A]$\Rightarrow$low mistakes[C]</font>.  
>
><font color="DeepSkyBlue">[3]</font>
><font color="OrangeRed">Low Mistakes$\Rightarrow$Doing Well</font>  
>&#10112;the beginning:  
>Given that you have algorithm [C] that bounds <font color="RosyBrown">the mistake pulls of $\varepsilon$ sub-optimal arms</font> in $T(K,\varepsilon,\delta)$, with high probability $1-\delta$, polynominal in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$.  
>&#10113;next target:  
>We'd like to build another algorithm that brings you the <font color="#9300FF">$\varepsilon'$ nearly maximized reward</font>, in other words, <font color="#9300FF">$\varepsilon'$ close to optimal per step reward</font>, with high probability $1-\delta'$, polynominal in $K$,$\frac {1}{\varepsilon'}$,$\frac {1}{\delta'}$.  
>
>proof:  
>$\Rightarrow$first by using algorithm [C], $T(K,\varepsilon,\delta)$ is the upper bound for mistake pulls, beyond this upper bound, you can do a lot pulls of correctness, say $T'(K,\varepsilon',\delta')$, take it as $n$, far more than this upper bound.  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2019-09-04-rl-explore-exploit-part2-few-mistake-do-well.png "equal")
>
>All the rest in $T'(K,\varepsilon',\delta')$ beyond $T(K,\varepsilon,\delta)$ have to be $\varepsilon'$ near optimal.  
>
>$\Rightarrow$here is one hint to remind you that <font color="RoyalBlue">from the point that you have pulled up to $T(K,\varepsilon,\delta)$ times, do you just start to make the pulls of right arm?</font>  
>
><font color="DeepSkyBlue">No, it's just [C] constraints the mistake pulls in $T(K,\varepsilon,\delta)$.</font>  
>
>At this point, <font color="OrangeRed">the distance from optimal reward would be greater than $\varepsilon$</font>.  
>
>$\Rightarrow$this is a <font color="OrangeRed">Bernoulli bandit, each pull gives you either zero or one paid</font>.  So, if you accumulate mistake pulls up to <font color="RosyBrown">$T(K,\varepsilon,\delta)$</font> times, the <font color="RosyBrown">total fail cases of mistake paid</font> is <font color="RosyBrown">$T(K,\varepsilon,\delta)\cdot 1$</font>, we take it as $m$.  At this moment, <font color="DeepSkyBlue">it is just $\varepsilon$ close to optimal, bypass the mistakes</font>.  
>
>$\Rightarrow$the <font color="RosyBrown">total sub-optimality over the end, left steps</font> is $m+\varepsilon\cdot(n-m)$.  And the <font color="RosyBrown">average per step sub-optimality</font> is $\frac {m+\varepsilon\cdot(n-m)}{n}$, where this <font color="RosyBrown">average per step sub-optimality is greater than the distance to the optimal reward</font>, that is to say $\varepsilon'\leq\frac {m+\varepsilon\cdot(n-m)}{n}$.  Ideally, <font color="Red">we want the distance $\varepsilon'$ optimal is smaller than the distance to the pull of mistake arms.</font>  
>
>$\Rightarrow$if we take $\varepsilon$=$\frac {\varepsilon'}{2}$ to try to further narrow down the mistake pulls, above inequality becomes:  
>$\varepsilon'\geq\frac {m+\frac {\varepsilon'}{2}\cdot(n-m)}{n}$, then  
><font color="DeepPink">$n\geq m\cdot(\frac {2}{\varepsilon'}-1)$</font>  
>
>It is just the quantity that $n$ should have been to make algorithm [B] validate or concrete.  We thus prove <font color="OrangeRed">low mistakes[C]$\Rightarrow$doing well[B]</font>.
>
><font color="DeepSkyBlue">[4]</font>
><font color="OrangeRed">Doing Well$\Rightarrow$Find Best</font>  
>&#10112;the beginning:  
>Given that you have algorithm [B] that can <font color="#9300FF">find the $\varepsilon$ nearly maximized reward</font>, with high probability $1-\delta$, after it has been executed over the number of steps $T(K,\varepsilon,\delta)$, which is polynominal in $K$,$\frac {1}{\varepsilon}$,$\frac {1}{\delta}$.  
>&#10113;next target:  
>We want to build an algorithm that can identify $\varepsilon'$ close to optimal arm, with high probability $1-\delta'$, after it has been pulled over $T'(K,\varepsilon',\delta')$ times.  
>
>proof:  
>$\Rightarrow$first by using algorithm [B], we do get the <font color="#9300FF">$\varepsilon$ nearly maximized reward</font> arm, that's the per step <font color="#9300FF">$\varepsilon$ nearly optimal reward</font> arm.  
>
>If we keep following <font color="OrangeRed">the probability distribution</font> of this pull, I will get <font color="OrangeRed">the same per step average</font>.  
>
>$\Rightarrow$next to introduce <font color="RosyBrown">$e_{i}$</font>, <font color="RosyBrown">the sub-optimality for the error of $arm_{i}$</font>, that is you should have selected $arm_{i}$, and you havn't during the execution of algorithm [C].  
>
>Such arm is chosen at least $\frac {1}{K}$ of the time, ideally, <font color="RosyBrown">we want the error of sub-optimality is as small as possible</font>, so the per step optimality $\varepsilon$ should be greater than or equal to $\frac {1}{K}\cdot e_{i}$, that is $\varepsilon\geq\frac {1}{K}\cdot e_{i}$.  
>
>$\Rightarrow$we also want <font color="RosyBrown">this error of sub-optimality to be smaller than</font> $\varepsilon'$, <font color="RosyBrown">the distance to the most optimal arm</font>, $\varepsilon'\geq e_{i}$, which is the constraint.  
>$\varepsilon\geq\frac {1}{K}\cdot e_{i}$...&#10112;  
>$\varepsilon'\geq e_{i}$...&#10113;  
>
>We relate $\varepsilon'$ to $\varepsilon$ by taking $\frac {\varepsilon'}{K}$=$\varepsilon$ in &#10112;, then &#10113; holds, everything just holds.  
>
>$\Rightarrow$we just prove <font color="OrangeRed">doing well[B]$\Rightarrow$find best[A]</font>, <font color="DeepPink">from the point we are $\varepsilon$ close to optimal reawrd, we can identify $\varepsilon'$ close to most optimal arm, where $\varepsilon'\leq\varepsilon$</font>.  

### Put It Together
>Here we can tell that  
>&#10112;algorithm [A] is for <font color="Red">exploitation</font>, to get <font color="Red">better estimate</font> of optimal arm.  
>&#10113;algorithm [B] is for <font color="Red">exploration</font>, to have <font color="Red">better rewards</font>.  
>&#10114;algorithm [C] is a <font color="RosyBrown">mistake bound</font>.  
>
>Put it all together, we know that <font color="DeepPink">we need exploration and exploitation interchangeable</font>.  

### Addendum
>&#10112;[Exploring Exploration, Charles IsBell, Michael Littman, Reinforcement Learning By Georgia Tech(CS8803)](https://classroom.udacity.com/courses/ud600/lessons/4402978778/concepts/44548888230923)  

<!-- Γ -->
<!-- \Omega -->
<!-- \cap intersection -->
<!-- \cup union -->
<!-- \frac{\Gamma(k + n)}{\Gamma(n)} \frac{1}{r^k}  -->
<!-- \mbox{\large$\vert$}\nolimits_0^\infty -->
<!-- \vert_0^\infty -->
<!-- \vert_{0.5}^{\infty} -->
<!-- &prime; ′ -->
<!-- &Prime; ″ -->
<!-- $E\lbrack X\rbrack$ -->
<!-- \overline{X_n} -->
<!-- \underset{Succss}P -->
<!-- \frac{{\overline {X_n}}-\mu}{S/\sqrt n} -->
<!-- \lim_{t\rightarrow\infty} -->
<!-- \int_{0}^{a}\lambda\cdot e^{-\lambda\cdot t}\operatorname dt -->
<!-- \Leftrightarrow -->
<!-- \prod_{v\in V} -->
<!-- \subset -->
<!-- \subseteq -->
<!-- \varnothing -->
<!-- \perp -->
<!-- \overset\triangle= -->
<!-- \left|X\right| -->
<!-- \xrightarrow{r_t} -->
<!-- \left\|?\right\| => ||?||-->
<!-- \left|?\right| => |?|-->
<!-- \lbrack BQ\rbrack => [BQ] -->
<!-- \subset -->
<!-- \subseteq -->

<!-- Notes -->
<!-- <font color="OrangeRed">items, verb, to make it the focus, mathematic expression</font> -->
<!-- <font color="Red">KKT</font> -->
<!-- <font color="Red">SMO heuristics</font> -->
<!-- <font color="Red">F</font> distribution -->
<!-- <font color="Red">t</font> distribution -->
<!-- <font color="DeepSkyBlue">suggested item, soft item</font> -->
<!-- <font color="RoyalBlue">old alpha, quiz, example</font> -->
<!-- <font color="Green">new alpha</font> -->

<!-- <font color="#C20000">conclusion, finding</font> -->
<!-- <font color="DeepPink">positive conclusion, finding</font> -->
<!-- <font color="RosyBrown">negative conclusion, finding</font> -->

<!-- <font color="#00ADAD">policy</font> -->
<!-- <font color="#6100A8">full observable</font> -->
<!-- <font color="#FFAC12">partial observable</font> -->
<!-- <font color="#EB00EB">stochastic</font> -->
<!-- <font color="#8400E6">state transition</font> -->
<!-- <font color="#D600D6">discount factor gamma $\gamma$</font> -->
<!-- <font color="#D600D6">$V(S)$</font> -->
<!-- <font color="#9300FF">immediate reward R(S)</font> -->

<!-- ### <font color="RoyalBlue">Example</font>: Illustration By Rainy And Sunny Days In One Week -->
<!-- <font color="RoyalBlue">[Question]</font> -->
<!-- <font color="DeepSkyBlue">[Answer]</font> -->

<!-- <font color="Brown">Notes::mjtsai1974</font> -->

<!-- 
[1]Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?</font>  
-->

<!--
><font color="DeepSkyBlue">[Notes]</font>
><font color="OrangeRed">Why at this moment, the Poisson and exponential probability come out with different result?</font>  
-->

<!-- https://www.medcalc.org/manual/gamma_distribution_functions.php -->
<!-- https://www.statlect.com/probability-distributions/student-t-distribution#hid5 -->
<!-- http://www.wiris.com/editor/demo/en/ -->