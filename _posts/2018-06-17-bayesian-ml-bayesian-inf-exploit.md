---
layout: post
title: The Bayesian Inference Exploitation
---

## Prologue To The <font color="Red">Bayesian Inference Exploitation</font>
<p class="message">
<font color="DeepPink">Bayesian inference is resembling gradient descendent approach to guide the experiment to the target satisfication</font>, they are not the same, but <font color="DeepPink">alike</font>!
</p>

### Prepare to Exploit The Bayesian Inference
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">ReCap The bayesian inference</font>  
>We can characterize how one’s beliefs ought to change when new information is gained.
>
>Remember the illustration example in the [The Bayesian Thinking]({{ site.github.repo }}{{ site.baseurl }}/2018/06/11/bayesian-ml-bayesian-think/), I have show you by contiguous update the given prior by the estimated posterior would we obtain the desired result by reinforcement.  
>
><font color="DeepSkyBlue">[2]</font>
><font color="OrangeRed">The question in mind</font>  
>From #1, #2, #3 tests, by feeding the prior with the positive posterior estimation, then we get almost $100\%$ in $P(Cancer\vert Malignant)$.  Below are the unknowns:  
>&#10112;<font color="OrangeRed">how, if we update the given prior several times with the positive posterior estimation, $P(Cancer\vert Malignant)$, laterly make contiguous negative posterior estimation, $P(Cancer\vert Benign)$?</font>  
>&#10113;will we have the chance to increase the negative posterior estimation, in its probability, $P(Cancer\vert Benign)$?  
>&#10114;for the answer of yes and no, could we describe the exact trend of $P(Cancer\vert Benign)$?  
>&#10115;or, is there some condition far beyond our naive beliefs that prior updated by some amount of positive posterior, we can concrete the negative posterior?  
>
><font color="DeepSkyBlue">[3]</font>
><font color="OrangeRed">The stress test</font>  
>Below is my stress test <font color="DeepSkyBlue">flow</font>:  
>&#10112;the rule of updating prior by the estimated posterior remains the same, when we make estimation of $P(Cancer\vert Malignant)$ and $P(Cancer\vert Benigh)$, at the end of each test, we do the update job.  
>&#10113;by executing $P(Cancer\vert Malignant)$ in $i$ times, next to execute $P(Cancer\vert Benign)$ over $100-i$, where $i$=$1$ to $100$.  
>&#10114;below graph exhibits the result of $P(Cancer\vert Malignant)$x${i}$, $P(Cancer\vert Benign)$x$(100-i)$ for the first 12 tests:  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-byreinforce.png "summary")
>The results are depicting from the left, top to the right, low in orders.  For the simplicity, I denote $P(C\vert T)$ as $P(Cancer\vert Malignant)$, $P(Cancer\vert F)$ as $P(Cancer\vert Benign)$.  
>&#10115;trivially, $P(C\vert T)$x$9$ to $P(C\vert T)$x$10$ is the major point, <font color="#C20000">after positive posterior over $10$ times, the following negative posterior won't be decreased, and keeps the same result as the latest positive posterior!</font>  

### <font color="DeepSkyBlue">Exploit</font> The Bayesian Inference
>Next we investigate what positive posterior over $10$ times has put inside the whole process, such that the following up $90$ negative posteriors would not downgrade the probabilistity of $P(Cancer\vert Benigh)$.  
>
>[1]By comparing the statistical log info of $P(C\vert T)$x$9$ and $P(C\vert T)$x$10$  
>The test is <font color="OrangeRed">0-index based</font> in my log, so you see run 9 is actually the 10-th execution of the test:  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-9-vs-10.png "9-vs-10")
>&#10112;the log comparison reveals that the run #9 in $P(C\vert T)$x$9$, the left side, says $P(Cancer\vert Benign)$ is almost $1$, but <font color="RosyBrown">not</font> yet!  
>&#10113;the right side result, the run #9 in $P(C\vert T)$x$10$ says $P(Cancer\vert Malignant)$ <font color="DeepPink">is equivalent to $1$!</font>  
>&#10114;the left side $P(C\vert F)$x$9$ has its $P(Cancer)$ and the total probability of $P(Malignant)$ downgraded, since the negative posterior is thus estimated:  
>$P(Cancer\vert Benign)$=$\frac {P(Benign\vert Cancer)\cdot P(Cancer)}{P(Benign)}$  
>$P(Benign)$=$P(Benign\vert Cancer)\cdot P(Cancer)$  
>$\;\;\;\;$+$P(Benign\vert Free)\cdot P(Free)$  
><font color="#C20000">The $P(Free)$ and $P(Benign)$ are compounded slowly increasing, the root cause to gradually decrease $P(Cancer\vert Benign)$!!</font>  
>&#10115;<font color="DeepPink">the right side $P(C\vert T)$x$10$, from the run #9(10-th) test of $P(Cancer\vert Malignant)$, we have $P(Cancer)$=$1$, $P(Free)$=$0$ keep invariant change in the total probability of $P(Malignant)$ and $P(Benign)$.</font>  
>&#10116;continue to inspect the log comparison result, we can see that in the series of $P(C\vert T)$x$9$, $P(C\vert F)$x$91$, the negative posterior becomes smaller and finally to 0; nevertheless, the $P(C\vert T)$x$10$, $P(C\vert F)$x$90$, we have $P(Cancer\vert Benign)$=$1$ all the way to the test end.  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-9-vs-10-cont.png "9-vs-10 cont")
>
>[2]Deeper inside $P(C\vert T)$x$9$ and $P(C\vert T)$x$10$  
>To make my findings concrete, the statistical summary of the 2 series are given in below graphs:  
>&#10112;below graph illustrates the consistency of my finding is of no doubt in $P(C\vert T)$x$9$.  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-c-b-9.png "P(C|T)x9")
>&#10113;the same in $P(C\vert T)$x$10$.  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-c-b-10.png "P(C|T)x10")
>
>[3]Other testing reports  
>At the end of this article, I depict all stress test I have done.  
>&#10112;it is the result of $P(Cancer\vert Malignant)$x$100$:  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-c-m-all.png "P(C|T)x100")
>&#10113;it is the result of $P(Cancer\vert Benign)$x$100$:  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-c-b-all.png "P(C|F)x100")
>&#10114;below exhibits the result in the series $P(Cancer\vert Malignant)$x$50$, then $P(Cancer\vert Benign)$x$50$:  
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-c-m-50.png "P(C|T)x50")
>&#10115;the result in the series $P(Cancer\vert Malignant)$,then $P(Cancer\vert Benign)$ as a whole the pattern for the behavior in the rest process:  
>You can tell that the patterned toggling exist for all terms of probability.   
![]({{ site.github.repo }}{{ site.baseurl }}/images/pic/2018-06-17-bayesian-ml-bayesian-inf-exploit-c-m-c-b.png "P(C|T),P(C|F)")
>
>All above reports are generated by my [Python simulator](https://mjtsai1974.github.io/DevBlog/template/BayesianInferPriorUpdating.py).  The related logs are downloaded here, [$P(C\vert T)$x$9$](https://mjtsai1974.github.io/DevBlog/template/P_Cancer_Malignant_9.txt) and [$P(C\vert T)$x$10$](https://mjtsai1974.github.io/DevBlog/template/P_Cancer_Malignant_10.txt).

<!--
[1]What is a Bayesian network?
Bayes theorem offers a fundamental mechanism for changing your opinion in the light of evidence. This is what Bayesian networks are about.

https://www.quora.com/What-is-a-Bayesian-network

[2]What are the relationships of Bayes' theorem, Bayesian inference, Naive Bayes, and Bayesian network (in simple English)?
[2.1]Bayesianism is an approach to systematizing reasoning under uncertainty.
[2.2]We can characterize how one’s beliefs ought to change when new information is gained.
[2.3]If we observe the truth or falsity of a relevant event, we can then use Bayes’ theorem to revise our probability assessment for other related events. This is called Bayesian inference.
[2.4]If we are thinking about a complex situation, in which our probability for events depend upon various others, we can use a Bayesian network (also called Bayes net) to represent what we believe. 
[2.5]In a Bayes net, there are nodes connected by arrows. Each node is the probability of an event. An arrow from event A to event B means that our probability of B depends on our probability of A. 
[2.6]Naive Bayes refers to a particularly simple form of a Bayes net, where your event of interest depends on other things, but none of them depends on one another.

https://www.quora.com/What-are-the-relationships-of-Bayes-theorem-Bayesian-inference-Naive-Bayes-and-Bayesian-network-in-simple-English

[3]How does Bayesian networks work?
[3.1]A Bayesian network is good at classifying based on observations.
[3.2]Therefore you can make a network that models relations between events in the present situation, symptoms of these and potential future effects. The BN would then be able to classify the present situation and hence predict future events with a probability.
[3.3]You can do unsupervised learning with a BN from a dataset and allow the learning algorithm to find both structure and probabilities.
[3.4]you can also do supervised learning with a BN, aiding the learning algorithm with a priori knowledge about relations and probabilities in the model. Here, results should become better than ANN and SVM.
[3.5]A BN is a white box approach where you can represent and evaluate the structure of the model explicitly whereas ANN and SVM are black box approaches where you really don't know why you get your results. This puts a limit to how good they can become.

https://www.quora.com/How-does-Bayesian-networks-work

[4]What is Bayesian machine learning?
[4.1]Machine learning is a set of methods for creating models that describe or predicting something about the world. It does so by learning those models from data.
[4.2]Bayesian machine learning allows us to encode our prior beliefs about what those models should look like, independent of what the data tells us. This is especially useful when we don’t have a ton of data to confidently learn our model.

https://www.quora.com/What-is-Bayesian-machine-learning

[5]What does Bayesian networks mean in Machine Learning?
[5.1]A Bayesian network essentially has random variables, and a graph structure that encodes the dependencies between the variables.
[5.2]A Bayesian network is a statistical model which connects random variables with their conditional probabilities. Bayes' theorem is used for the computation of probabilities in the network.

https://www.quora.com/What-does-Bayesian-networks-mean-in-Machine-Learning
-->

<!-- Γ -->
<!-- \Omega -->
<!-- \cap intersection -->
<!-- \cup union -->
<!-- \frac{\Gamma(k + n)}{\Gamma(n)} \frac{1}{r^k}  -->
<!-- \mbox{\large$\vert$}\nolimits_0^\infty -->
<!-- \vert_0^\infty -->
<!-- \vert_{0.5}^{\infty} -->
<!-- &prime; ′ -->
<!-- &Prime; ″ -->
<!-- $E\lbrack X\rbrack$ -->
<!-- \widehat X -->
<!-- \overline{X_n} -->
<!-- \underset{w_{real}}{maxarg} -->
<!-- \underset{Succss}P -->
<!-- \frac{{\overline {X_n}}-\mu}{S/\sqrt n} -->
<!-- \lim_{t\rightarrow\infty} -->
<!-- \int_{0}^{a}\lambda\cdot e^{-\lambda\cdot t}\operatorname dt -->

<!-- Notes -->
<!-- <font color="OrangeRed">items, verb, to make it the focus</font> -->
<!-- <font color="Red">KKT</font> -->
<!-- <font color="Red">SMO heuristics</font> -->
<!-- <font color="Red">F</font> distribution -->
<!-- <font color="Red">t</font> distribution -->
<!-- <font color="DeepSkyBlue">suggested item, soft item</font> -->
<!-- <font color="RoyalBlue">old alpha, quiz, example</font> -->
<!-- <font color="Green">new alpha</font> -->

<!-- <font color="#C20000">conclusion, finding, more details</font> -->
<!-- <font color="DeepPink">positive conclusion, finding</font> -->
<!-- <font color="RosyBrown">negative conclusion, finding</font> -->

<!-- <font color="#00ADAD">policy</font> -->
<!-- <font color="#6100A8">full observable</font> -->
<!-- <font color="#FFAC12">partial observable</font> -->
<!-- <font color="#EB00EB">stochastic</font> -->
<!-- <font color="#8400E6">state transition</font> -->
<!-- <font color="#D600D6">discount factor gamma $\gamma$</font> -->
<!-- <font color="#D600D6">$V(S)$</font> -->
<!-- <font color="#9300FF">immediate reward R(S)</font> -->

<!-- ### <font color="RoyalBlue">Example</font>: Illustration By Rainy And Sunny Days In One Week -->
<!-- <font color="RoyalBlue">[Question]</font> -->
<!-- <font color="DeepSkyBlue">[Answer]</font> -->

<!-- 
[1]Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?
><font color="DeepSkyBlue">[1]</font>
><font color="OrangeRed">Given the vehicles pass through a highway toll station is $6$ per minute, what is the probability that no cars within $30$ seconds?</font>  
-->

<!--
<table>
  <tr>
    <td>項次</td>
    <td>品名</td>
    <td>描述</td>
  </tr>
  <tr>
    <td>1</td>
    <td>iPhone 5</td>
    <td>iPhone 6 > 5</td>
  </tr>
</table>

<TABLE border="1">
  <TR>
    <TD width="50px">A</TD>
    <TD width="100px">B</TD>
  </TR>
</TABLE>

<TABLE border="1">
  <TR>
    <TD width="50px">A</TD>
    <TD width="100px">B</TD>
  </TR>
  <TR>
    <TD>C</TD>
    <TD>D</TD>
  </TR>
</TABLE>

<TABLE border="1">
  <COL width="50px">
  <COL width="100px">
  <COL width="50px">
  <TR>
    <TD colspan="2">A</TD>
    <TD>B</TD>
  </TR>
  <TR>
    <TD>C</TD>
    <TD>D</TD>
    <TD>E</TD>
  </TR>
</TABLE>
-->

<!--
name | age
---- | ---
LearnShare | 12
Mike |  32

| left | center | right |
| :--- | :----: | ----: |
| aaaa | bbbbbb | ccccc |
| a    | b      | c     |
-->

<!-- https://www.medcalc.org/manual/gamma_distribution_functions.php -->
<!-- https://www.statlect.com/probability-distributions/student-t-distribution#hid5 -->
<!-- http://www.wiris.com/editor/demo/en/ -->
<!-- http://www.astroml.org/book_figures/chapter3/fig_gaussian_distribution.html -->